---
layout: post
excerpt: Introduction to pseudo labeling technique
comments: true
images:
  - url: assets/pseudo-labels-title.png
---

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p> A major requirement in all supervised deep learning algorithms is huge amount of data for training the model. In many applications collecting large dataset is not feasible or can be expensive or time consuming as we have to label the data manually. To overcome this challenge, we have a semi-supervised learning technique named “pseudo labeling”, which can be briefly described as following: </p>
<ul>
  <li> Train an initial model on the available labeled data </li>
  <li> Make predictions on unlabeled data (or unseen test data) and add the confident predictions with its predicted labels into training data </li>
  <li> Train the new model with combined data (previous training data + confident predictions or pseudo labeled data) </li>
  <li> Repeat this procedure multiple times (step 2 & 3) if required. </li>
</ul>

<img src="assets/pseudo-labels.png" height=300> </img>

<center> Reference: <a href="https://mc.ai/pseudo-labeling/"> https://mc.ai/pseudo-labeling/ </a> </center>

<h2> When and why does Pseudo labeling work? </h2>

<p> Pseudo labeling works when the structure of the data satisfies the following assumptions made for pseudo labeling: </p>

<ul>
  <li> <b> Continuity Assumption: </b> The data points which are close to each other are more likely to fall under the same class. </li>
  <li> <b> Cluster Assumption: </b> The data points are clustered according to class. So, nearby points have the same class, and points in different classes are more widely separated. </li>
</ul>

<p> Therefore, pseudo labeling to work, our model should be able to learn the cluster structure of the entire dataset from the initial training data. When we assign a pseudo label, we are using the cluster structure that the model has learned to infer labels for the unlabeled data. As the training progresses, the learned cluster structure is improved using the unlabeled data. If the initial labeled data is too small in size or does not contain data points for all the classes or it contains outliers, pseudo labeling will likely assign incorrect labels to the unlabeled points. </p>

<h2> Implementation Details: </h2>

<p> I learned this technique from a recent Kaggle competition in which we were supposed to detect multiple wheat heads in the given image of wheat crop (an object detection problem). In this, the training dataset contained ~3000 training images each having multiple wheat heads/no wheat heads. Here is the link to this competition: <a href="https://www.kaggle.com/c/global-wheat-detection"> https://www.kaggle.com/c/global-wheat-detection </a> </p>

<p> In this competition, I implemented a FasterRCNN with 2 and 3 rounds of pseudo labeling and it significantly improved my score. Here is the link of my kernel having complete implementation in PyTorch: <a href="https://www.kaggle.com/kaushal2896/faster-rcnn-2-rounds-of-pseudo-labeling"> https://www.kaggle.com/kaushal2896/faster-rcnn-2-rounds-of-pseudo-labeling </a> </p>
